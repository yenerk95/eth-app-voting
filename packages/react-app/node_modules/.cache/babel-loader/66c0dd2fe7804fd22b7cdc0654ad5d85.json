{"ast":null,"code":"'use strict';\n\nconst {\n  DAGLink,\n  DAGNode\n} = require('ipld-dag-pb');\n\nconst CID = require('cids');\n\nconst log = require('debug')('ipfs:mfs:core:utils:add-link');\n\nconst UnixFS = require('ipfs-unixfs');\n\nconst DirSharded = require('ipfs-unixfs-importer/src/dir-sharded');\n\nconst {\n  updateHamtDirectory,\n  recreateHamtLevel,\n  createShard,\n  toPrefix,\n  addLinksToHamtBucket\n} = require('./hamt-utils');\n\nconst errCode = require('err-code');\n\nconst mc = require('multicodec');\n\nconst mh = require('multihashing-async').multihash;\n\nconst last = require('it-last');\n\nconst addLink = async (context, options) => {\n  if (!options.parentCid && !options.parent) {\n    throw errCode(new Error('No parent node or CID passed to addLink'), 'EINVALIDPARENT');\n  }\n\n  if (options.parentCid && !CID.isCID(options.parentCid)) {\n    throw errCode(new Error('Invalid CID passed to addLink'), 'EINVALIDPARENTCID');\n  }\n\n  if (!options.parent) {\n    log(`Loading parent node ${options.parentCid}`);\n    options.parent = await context.ipld.get(options.parentCid);\n  }\n\n  if (!options.cid) {\n    throw errCode(new Error('No child cid passed to addLink'), 'EINVALIDCHILDCID');\n  }\n\n  if (!options.name) {\n    throw errCode(new Error('No child name passed to addLink'), 'EINVALIDCHILDNAME');\n  }\n\n  if (!CID.isCID(options.cid)) {\n    options.cid = new CID(options.cid);\n  }\n\n  if (!options.size && options.size !== 0) {\n    throw errCode(new Error('No child size passed to addLink'), 'EINVALIDCHILDSIZE');\n  }\n\n  const meta = UnixFS.unmarshal(options.parent.Data);\n\n  if (meta.type === 'hamt-sharded-directory') {\n    log('Adding link to sharded directory');\n    return addToShardedDirectory(context, options);\n  }\n\n  if (options.parent.Links.length >= options.shardSplitThreshold) {\n    log('Converting directory to sharded directory');\n    return convertToShardedDirectory(context, { ...options,\n      mtime: meta.mtime,\n      mode: meta.mode\n    });\n  }\n\n  log(`Adding ${options.name} (${options.cid}) to regular directory`);\n  return addToDirectory(context, options);\n};\n\nconst convertToShardedDirectory = async (context, options) => {\n  const result = await createShard(context, options.parent.Links.map(link => ({\n    name: link.Name,\n    size: link.Tsize,\n    cid: link.Hash\n  })).concat({\n    name: options.name,\n    size: options.size,\n    cid: options.cid\n  }), options);\n  log(`Converted directory to sharded directory ${result.cid}`);\n  return result;\n};\n\nconst addToDirectory = async (context, options) => {\n  options.parent.rmLink(options.name);\n  options.parent.addLink(new DAGLink(options.name, options.size, options.cid));\n  const node = UnixFS.unmarshal(options.parent.Data);\n\n  if (node.mtime) {\n    // Update mtime if previously set\n    node.mtime = new Date();\n    options.parent = new DAGNode(node.marshal(), options.parent.Links);\n  }\n\n  const hashAlg = mh.names[options.hashAlg]; // Persist the new parent DAGNode\n\n  const cid = await context.ipld.put(options.parent, mc.DAG_PB, {\n    cidVersion: options.cidVersion,\n    hashAlg,\n    onlyHash: !options.flush\n  });\n  return {\n    node: options.parent,\n    cid,\n    size: options.parent.size\n  };\n};\n\nconst addToShardedDirectory = async (context, options) => {\n  const {\n    shard,\n    path\n  } = await addFileToShardedDirectory(context, options);\n  const result = await last(shard.flush('', context.block));\n  const node = await context.ipld.get(result.cid); // we have written out the shard, but only one sub-shard will have been written so replace it in the original shard\n\n  const oldLink = options.parent.Links.find(link => link.Name.substring(0, 2) === path[0].prefix);\n  const newLink = node.Links.find(link => link.Name.substring(0, 2) === path[0].prefix);\n\n  if (oldLink) {\n    options.parent.rmLink(oldLink.Name);\n  }\n\n  options.parent.addLink(newLink);\n  return updateHamtDirectory(context, options.parent.Links, path[0].bucket, options);\n};\n\nconst addFileToShardedDirectory = async (context, options) => {\n  const file = {\n    name: options.name,\n    cid: options.cid,\n    size: options.size\n  }; // start at the root bucket and descend, loading nodes as we go\n\n  const rootBucket = await recreateHamtLevel(options.parent.Links);\n  const node = UnixFS.unmarshal(options.parent.Data);\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: null,\n    parentKey: null,\n    path: '',\n    dirty: true,\n    flat: false,\n    mode: node.mode\n  }, options);\n  shard._bucket = rootBucket;\n\n  if (node.mtime) {\n    // update mtime if previously set\n    shard.mtime = new Date();\n  } // load subshards until the bucket & position no longer changes\n\n\n  const position = await rootBucket._findNewBucketAndPos(file.name);\n  const path = toBucketPath(position);\n  path[0].node = options.parent;\n  let index = 0;\n\n  while (index < path.length) {\n    const segment = path[index];\n    index++;\n    const node = segment.node;\n    const link = node.Links.find(link => link.Name.substring(0, 2) === segment.prefix);\n\n    if (!link) {\n      // prefix is new, file will be added to the current bucket\n      log(`Link ${segment.prefix}${file.name} will be added`);\n      index = path.length;\n      break;\n    }\n\n    if (link.Name === `${segment.prefix}${file.name}`) {\n      // file already existed, file will be added to the current bucket\n      log(`Link ${segment.prefix}${file.name} will be replaced`);\n      index = path.length;\n      break;\n    }\n\n    if (link.Name.length > 2) {\n      // another file had the same prefix, will be replaced with a subshard\n      log(`Link ${link.Name} ${link.Hash} will be replaced with a subshard`);\n      index = path.length;\n      break;\n    } // load sub-shard\n\n\n    log(`Found subshard ${segment.prefix}`);\n    const subShard = await context.ipld.get(link.Hash); // subshard hasn't been loaded, descend to the next level of the HAMT\n\n    if (!path[index]) {\n      log(`Loaded new subshard ${segment.prefix}`);\n      await recreateHamtLevel(subShard.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n      const position = await rootBucket._findNewBucketAndPos(file.name);\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: subShard\n      });\n      break;\n    }\n\n    const nextSegment = path[index]; // add next levels worth of links to bucket\n\n    await addLinksToHamtBucket(subShard.Links, nextSegment.bucket, rootBucket);\n    nextSegment.node = subShard;\n  } // finally add the new file into the shard\n\n\n  await shard._bucket.put(file.name, {\n    size: file.size,\n    cid: file.cid\n  });\n  return {\n    shard,\n    path\n  };\n};\n\nconst toBucketPath = position => {\n  let bucket = position.bucket;\n  let positionInBucket = position.pos;\n  const path = [{\n    bucket,\n    prefix: toPrefix(positionInBucket)\n  }];\n  bucket = position.bucket._parent;\n  positionInBucket = position.bucket._posAtParent;\n\n  while (bucket) {\n    path.push({\n      bucket,\n      prefix: toPrefix(positionInBucket)\n    });\n    positionInBucket = bucket._posAtParent;\n    bucket = bucket._parent;\n  }\n\n  path.reverse();\n  return path;\n};\n\nmodule.exports = addLink;","map":{"version":3,"sources":["/Users/yenerkaraca/Documents/GitHub/eth-app/node_modules/ipfs/src/core/components/files/utils/add-link.js"],"names":["DAGLink","DAGNode","require","CID","log","UnixFS","DirSharded","updateHamtDirectory","recreateHamtLevel","createShard","toPrefix","addLinksToHamtBucket","errCode","mc","mh","multihash","last","addLink","context","options","parentCid","parent","Error","isCID","ipld","get","cid","name","size","meta","unmarshal","Data","type","addToShardedDirectory","Links","length","shardSplitThreshold","convertToShardedDirectory","mtime","mode","addToDirectory","result","map","link","Name","Tsize","Hash","concat","rmLink","node","Date","marshal","hashAlg","names","put","DAG_PB","cidVersion","onlyHash","flush","shard","path","addFileToShardedDirectory","block","oldLink","find","substring","prefix","newLink","bucket","file","rootBucket","root","dir","parentKey","dirty","flat","_bucket","position","_findNewBucketAndPos","toBucketPath","index","segment","subShard","parseInt","push","pos","nextSegment","positionInBucket","_parent","_posAtParent","reverse","module","exports"],"mappings":"AAAA;;AAEA,MAAM;AACJA,EAAAA,OADI;AAEJC,EAAAA;AAFI,IAGFC,OAAO,CAAC,aAAD,CAHX;;AAIA,MAAMC,GAAG,GAAGD,OAAO,CAAC,MAAD,CAAnB;;AACA,MAAME,GAAG,GAAGF,OAAO,CAAC,OAAD,CAAP,CAAiB,8BAAjB,CAAZ;;AACA,MAAMG,MAAM,GAAGH,OAAO,CAAC,aAAD,CAAtB;;AACA,MAAMI,UAAU,GAAGJ,OAAO,CAAC,sCAAD,CAA1B;;AACA,MAAM;AACJK,EAAAA,mBADI;AAEJC,EAAAA,iBAFI;AAGJC,EAAAA,WAHI;AAIJC,EAAAA,QAJI;AAKJC,EAAAA;AALI,IAMFT,OAAO,CAAC,cAAD,CANX;;AAOA,MAAMU,OAAO,GAAGV,OAAO,CAAC,UAAD,CAAvB;;AACA,MAAMW,EAAE,GAAGX,OAAO,CAAC,YAAD,CAAlB;;AACA,MAAMY,EAAE,GAAGZ,OAAO,CAAC,oBAAD,CAAP,CAA8Ba,SAAzC;;AACA,MAAMC,IAAI,GAAGd,OAAO,CAAC,SAAD,CAApB;;AAEA,MAAMe,OAAO,GAAG,OAAOC,OAAP,EAAgBC,OAAhB,KAA4B;AAC1C,MAAI,CAACA,OAAO,CAACC,SAAT,IAAsB,CAACD,OAAO,CAACE,MAAnC,EAA2C;AACzC,UAAMT,OAAO,CAAC,IAAIU,KAAJ,CAAU,yCAAV,CAAD,EAAuD,gBAAvD,CAAb;AACD;;AAED,MAAIH,OAAO,CAACC,SAAR,IAAqB,CAACjB,GAAG,CAACoB,KAAJ,CAAUJ,OAAO,CAACC,SAAlB,CAA1B,EAAwD;AACtD,UAAMR,OAAO,CAAC,IAAIU,KAAJ,CAAU,+BAAV,CAAD,EAA6C,mBAA7C,CAAb;AACD;;AAED,MAAI,CAACH,OAAO,CAACE,MAAb,EAAqB;AACnBjB,IAAAA,GAAG,CAAE,uBAAsBe,OAAO,CAACC,SAAU,EAA1C,CAAH;AAEAD,IAAAA,OAAO,CAACE,MAAR,GAAiB,MAAMH,OAAO,CAACM,IAAR,CAAaC,GAAb,CAAiBN,OAAO,CAACC,SAAzB,CAAvB;AACD;;AAED,MAAI,CAACD,OAAO,CAACO,GAAb,EAAkB;AAChB,UAAMd,OAAO,CAAC,IAAIU,KAAJ,CAAU,gCAAV,CAAD,EAA8C,kBAA9C,CAAb;AACD;;AAED,MAAI,CAACH,OAAO,CAACQ,IAAb,EAAmB;AACjB,UAAMf,OAAO,CAAC,IAAIU,KAAJ,CAAU,iCAAV,CAAD,EAA+C,mBAA/C,CAAb;AACD;;AAED,MAAI,CAACnB,GAAG,CAACoB,KAAJ,CAAUJ,OAAO,CAACO,GAAlB,CAAL,EAA6B;AAC3BP,IAAAA,OAAO,CAACO,GAAR,GAAc,IAAIvB,GAAJ,CAAQgB,OAAO,CAACO,GAAhB,CAAd;AACD;;AAED,MAAI,CAACP,OAAO,CAACS,IAAT,IAAiBT,OAAO,CAACS,IAAR,KAAiB,CAAtC,EAAyC;AACvC,UAAMhB,OAAO,CAAC,IAAIU,KAAJ,CAAU,iCAAV,CAAD,EAA+C,mBAA/C,CAAb;AACD;;AAED,QAAMO,IAAI,GAAGxB,MAAM,CAACyB,SAAP,CAAiBX,OAAO,CAACE,MAAR,CAAeU,IAAhC,CAAb;;AAEA,MAAIF,IAAI,CAACG,IAAL,KAAc,wBAAlB,EAA4C;AAC1C5B,IAAAA,GAAG,CAAC,kCAAD,CAAH;AAEA,WAAO6B,qBAAqB,CAACf,OAAD,EAAUC,OAAV,CAA5B;AACD;;AAED,MAAIA,OAAO,CAACE,MAAR,CAAea,KAAf,CAAqBC,MAArB,IAA+BhB,OAAO,CAACiB,mBAA3C,EAAgE;AAC9DhC,IAAAA,GAAG,CAAC,2CAAD,CAAH;AAEA,WAAOiC,yBAAyB,CAACnB,OAAD,EAAU,EACxC,GAAGC,OADqC;AAExCmB,MAAAA,KAAK,EAAET,IAAI,CAACS,KAF4B;AAGxCC,MAAAA,IAAI,EAAEV,IAAI,CAACU;AAH6B,KAAV,CAAhC;AAKD;;AAEDnC,EAAAA,GAAG,CAAE,UAASe,OAAO,CAACQ,IAAK,KAAIR,OAAO,CAACO,GAAI,wBAAxC,CAAH;AAEA,SAAOc,cAAc,CAACtB,OAAD,EAAUC,OAAV,CAArB;AACD,CApDD;;AAsDA,MAAMkB,yBAAyB,GAAG,OAAOnB,OAAP,EAAgBC,OAAhB,KAA4B;AAC5D,QAAMsB,MAAM,GAAG,MAAMhC,WAAW,CAACS,OAAD,EAAUC,OAAO,CAACE,MAAR,CAAea,KAAf,CAAqBQ,GAArB,CAAyBC,IAAI,KAAK;AAC1EhB,IAAAA,IAAI,EAAEgB,IAAI,CAACC,IAD+D;AAE1EhB,IAAAA,IAAI,EAAEe,IAAI,CAACE,KAF+D;AAG1EnB,IAAAA,GAAG,EAAEiB,IAAI,CAACG;AAHgE,GAAL,CAA7B,EAItCC,MAJsC,CAI/B;AACTpB,IAAAA,IAAI,EAAER,OAAO,CAACQ,IADL;AAETC,IAAAA,IAAI,EAAET,OAAO,CAACS,IAFL;AAGTF,IAAAA,GAAG,EAAEP,OAAO,CAACO;AAHJ,GAJ+B,CAAV,EAQ5BP,OAR4B,CAAhC;AAUAf,EAAAA,GAAG,CAAE,4CAA2CqC,MAAM,CAACf,GAAI,EAAxD,CAAH;AAEA,SAAOe,MAAP;AACD,CAdD;;AAgBA,MAAMD,cAAc,GAAG,OAAOtB,OAAP,EAAgBC,OAAhB,KAA4B;AACjDA,EAAAA,OAAO,CAACE,MAAR,CAAe2B,MAAf,CAAsB7B,OAAO,CAACQ,IAA9B;AACAR,EAAAA,OAAO,CAACE,MAAR,CAAeJ,OAAf,CAAuB,IAAIjB,OAAJ,CAAYmB,OAAO,CAACQ,IAApB,EAA0BR,OAAO,CAACS,IAAlC,EAAwCT,OAAO,CAACO,GAAhD,CAAvB;AAEA,QAAMuB,IAAI,GAAG5C,MAAM,CAACyB,SAAP,CAAiBX,OAAO,CAACE,MAAR,CAAeU,IAAhC,CAAb;;AAEA,MAAIkB,IAAI,CAACX,KAAT,EAAgB;AACd;AACAW,IAAAA,IAAI,CAACX,KAAL,GAAa,IAAIY,IAAJ,EAAb;AAEA/B,IAAAA,OAAO,CAACE,MAAR,GAAiB,IAAIpB,OAAJ,CAAYgD,IAAI,CAACE,OAAL,EAAZ,EAA4BhC,OAAO,CAACE,MAAR,CAAea,KAA3C,CAAjB;AACD;;AAED,QAAMkB,OAAO,GAAGtC,EAAE,CAACuC,KAAH,CAASlC,OAAO,CAACiC,OAAjB,CAAhB,CAbiD,CAejD;;AACA,QAAM1B,GAAG,GAAG,MAAMR,OAAO,CAACM,IAAR,CAAa8B,GAAb,CAAiBnC,OAAO,CAACE,MAAzB,EAAiCR,EAAE,CAAC0C,MAApC,EAA4C;AAC5DC,IAAAA,UAAU,EAAErC,OAAO,CAACqC,UADwC;AAE5DJ,IAAAA,OAF4D;AAG5DK,IAAAA,QAAQ,EAAE,CAACtC,OAAO,CAACuC;AAHyC,GAA5C,CAAlB;AAMA,SAAO;AACLT,IAAAA,IAAI,EAAE9B,OAAO,CAACE,MADT;AAELK,IAAAA,GAFK;AAGLE,IAAAA,IAAI,EAAET,OAAO,CAACE,MAAR,CAAeO;AAHhB,GAAP;AAKD,CA3BD;;AA6BA,MAAMK,qBAAqB,GAAG,OAAOf,OAAP,EAAgBC,OAAhB,KAA4B;AACxD,QAAM;AACJwC,IAAAA,KADI;AACGC,IAAAA;AADH,MAEF,MAAMC,yBAAyB,CAAC3C,OAAD,EAAUC,OAAV,CAFnC;AAIA,QAAMsB,MAAM,GAAG,MAAMzB,IAAI,CAAC2C,KAAK,CAACD,KAAN,CAAY,EAAZ,EAAgBxC,OAAO,CAAC4C,KAAxB,CAAD,CAAzB;AACA,QAAMb,IAAI,GAAG,MAAM/B,OAAO,CAACM,IAAR,CAAaC,GAAb,CAAiBgB,MAAM,CAACf,GAAxB,CAAnB,CANwD,CAQxD;;AACA,QAAMqC,OAAO,GAAG5C,OAAO,CAACE,MAAR,CAAea,KAAf,CACb8B,IADa,CACRrB,IAAI,IAAIA,IAAI,CAACC,IAAL,CAAUqB,SAAV,CAAoB,CAApB,EAAuB,CAAvB,MAA8BL,IAAI,CAAC,CAAD,CAAJ,CAAQM,MADtC,CAAhB;AAGA,QAAMC,OAAO,GAAGlB,IAAI,CAACf,KAAL,CACb8B,IADa,CACRrB,IAAI,IAAIA,IAAI,CAACC,IAAL,CAAUqB,SAAV,CAAoB,CAApB,EAAuB,CAAvB,MAA8BL,IAAI,CAAC,CAAD,CAAJ,CAAQM,MADtC,CAAhB;;AAGA,MAAIH,OAAJ,EAAa;AACX5C,IAAAA,OAAO,CAACE,MAAR,CAAe2B,MAAf,CAAsBe,OAAO,CAACnB,IAA9B;AACD;;AAEDzB,EAAAA,OAAO,CAACE,MAAR,CAAeJ,OAAf,CAAuBkD,OAAvB;AAEA,SAAO5D,mBAAmB,CAACW,OAAD,EAAUC,OAAO,CAACE,MAAR,CAAea,KAAzB,EAAgC0B,IAAI,CAAC,CAAD,CAAJ,CAAQQ,MAAxC,EAAgDjD,OAAhD,CAA1B;AACD,CAtBD;;AAwBA,MAAM0C,yBAAyB,GAAG,OAAO3C,OAAP,EAAgBC,OAAhB,KAA4B;AAC5D,QAAMkD,IAAI,GAAG;AACX1C,IAAAA,IAAI,EAAER,OAAO,CAACQ,IADH;AAEXD,IAAAA,GAAG,EAAEP,OAAO,CAACO,GAFF;AAGXE,IAAAA,IAAI,EAAET,OAAO,CAACS;AAHH,GAAb,CAD4D,CAO5D;;AACA,QAAM0C,UAAU,GAAG,MAAM9D,iBAAiB,CAACW,OAAO,CAACE,MAAR,CAAea,KAAhB,CAA1C;AACA,QAAMe,IAAI,GAAG5C,MAAM,CAACyB,SAAP,CAAiBX,OAAO,CAACE,MAAR,CAAeU,IAAhC,CAAb;AAEA,QAAM4B,KAAK,GAAG,IAAIrD,UAAJ,CAAe;AAC3BiE,IAAAA,IAAI,EAAE,IADqB;AAE3BC,IAAAA,GAAG,EAAE,IAFsB;AAG3BnD,IAAAA,MAAM,EAAE,IAHmB;AAI3BoD,IAAAA,SAAS,EAAE,IAJgB;AAK3Bb,IAAAA,IAAI,EAAE,EALqB;AAM3Bc,IAAAA,KAAK,EAAE,IANoB;AAO3BC,IAAAA,IAAI,EAAE,KAPqB;AAQ3BpC,IAAAA,IAAI,EAAEU,IAAI,CAACV;AARgB,GAAf,EASXpB,OATW,CAAd;AAUAwC,EAAAA,KAAK,CAACiB,OAAN,GAAgBN,UAAhB;;AAEA,MAAIrB,IAAI,CAACX,KAAT,EAAgB;AACd;AACAqB,IAAAA,KAAK,CAACrB,KAAN,GAAc,IAAIY,IAAJ,EAAd;AACD,GA1B2D,CA4B5D;;;AACA,QAAM2B,QAAQ,GAAG,MAAMP,UAAU,CAACQ,oBAAX,CAAgCT,IAAI,CAAC1C,IAArC,CAAvB;AACA,QAAMiC,IAAI,GAAGmB,YAAY,CAACF,QAAD,CAAzB;AACAjB,EAAAA,IAAI,CAAC,CAAD,CAAJ,CAAQX,IAAR,GAAe9B,OAAO,CAACE,MAAvB;AACA,MAAI2D,KAAK,GAAG,CAAZ;;AAEA,SAAOA,KAAK,GAAGpB,IAAI,CAACzB,MAApB,EAA4B;AAC1B,UAAM8C,OAAO,GAAGrB,IAAI,CAACoB,KAAD,CAApB;AACAA,IAAAA,KAAK;AACL,UAAM/B,IAAI,GAAGgC,OAAO,CAAChC,IAArB;AAEA,UAAMN,IAAI,GAAGM,IAAI,CAACf,KAAL,CACV8B,IADU,CACLrB,IAAI,IAAIA,IAAI,CAACC,IAAL,CAAUqB,SAAV,CAAoB,CAApB,EAAuB,CAAvB,MAA8BgB,OAAO,CAACf,MADzC,CAAb;;AAGA,QAAI,CAACvB,IAAL,EAAW;AACT;AACAvC,MAAAA,GAAG,CAAE,QAAO6E,OAAO,CAACf,MAAO,GAAEG,IAAI,CAAC1C,IAAK,gBAApC,CAAH;AACAqD,MAAAA,KAAK,GAAGpB,IAAI,CAACzB,MAAb;AAEA;AACD;;AAED,QAAIQ,IAAI,CAACC,IAAL,KAAe,GAAEqC,OAAO,CAACf,MAAO,GAAEG,IAAI,CAAC1C,IAAK,EAAhD,EAAmD;AACjD;AACAvB,MAAAA,GAAG,CAAE,QAAO6E,OAAO,CAACf,MAAO,GAAEG,IAAI,CAAC1C,IAAK,mBAApC,CAAH;AACAqD,MAAAA,KAAK,GAAGpB,IAAI,CAACzB,MAAb;AAEA;AACD;;AAED,QAAIQ,IAAI,CAACC,IAAL,CAAUT,MAAV,GAAmB,CAAvB,EAA0B;AACxB;AACA/B,MAAAA,GAAG,CAAE,QAAOuC,IAAI,CAACC,IAAK,IAAGD,IAAI,CAACG,IAAK,mCAAhC,CAAH;AACAkC,MAAAA,KAAK,GAAGpB,IAAI,CAACzB,MAAb;AAEA;AACD,KA9ByB,CAgC1B;;;AACA/B,IAAAA,GAAG,CAAE,kBAAiB6E,OAAO,CAACf,MAAO,EAAlC,CAAH;AACA,UAAMgB,QAAQ,GAAG,MAAMhE,OAAO,CAACM,IAAR,CAAaC,GAAb,CAAiBkB,IAAI,CAACG,IAAtB,CAAvB,CAlC0B,CAoC1B;;AACA,QAAI,CAACc,IAAI,CAACoB,KAAD,CAAT,EAAkB;AAChB5E,MAAAA,GAAG,CAAE,uBAAsB6E,OAAO,CAACf,MAAO,EAAvC,CAAH;AACA,YAAM1D,iBAAiB,CAAC0E,QAAQ,CAAChD,KAAV,EAAiBoC,UAAjB,EAA6BW,OAAO,CAACb,MAArC,EAA6Ce,QAAQ,CAACF,OAAO,CAACf,MAAT,EAAiB,EAAjB,CAArD,CAAvB;AAEA,YAAMW,QAAQ,GAAG,MAAMP,UAAU,CAACQ,oBAAX,CAAgCT,IAAI,CAAC1C,IAArC,CAAvB;AAEAiC,MAAAA,IAAI,CAACwB,IAAL,CAAU;AACRhB,QAAAA,MAAM,EAAES,QAAQ,CAACT,MADT;AAERF,QAAAA,MAAM,EAAExD,QAAQ,CAACmE,QAAQ,CAACQ,GAAV,CAFR;AAGRpC,QAAAA,IAAI,EAAEiC;AAHE,OAAV;AAMA;AACD;;AAED,UAAMI,WAAW,GAAG1B,IAAI,CAACoB,KAAD,CAAxB,CApD0B,CAsD1B;;AACA,UAAMrE,oBAAoB,CAACuE,QAAQ,CAAChD,KAAV,EAAiBoD,WAAW,CAAClB,MAA7B,EAAqCE,UAArC,CAA1B;AAEAgB,IAAAA,WAAW,CAACrC,IAAZ,GAAmBiC,QAAnB;AACD,GA5F2D,CA8F5D;;;AACA,QAAMvB,KAAK,CAACiB,OAAN,CAActB,GAAd,CAAkBe,IAAI,CAAC1C,IAAvB,EAA6B;AACjCC,IAAAA,IAAI,EAAEyC,IAAI,CAACzC,IADsB;AAEjCF,IAAAA,GAAG,EAAE2C,IAAI,CAAC3C;AAFuB,GAA7B,CAAN;AAKA,SAAO;AACLiC,IAAAA,KADK;AACEC,IAAAA;AADF,GAAP;AAGD,CAvGD;;AAyGA,MAAMmB,YAAY,GAAIF,QAAD,IAAc;AACjC,MAAIT,MAAM,GAAGS,QAAQ,CAACT,MAAtB;AACA,MAAImB,gBAAgB,GAAGV,QAAQ,CAACQ,GAAhC;AACA,QAAMzB,IAAI,GAAG,CAAC;AACZQ,IAAAA,MADY;AAEZF,IAAAA,MAAM,EAAExD,QAAQ,CAAC6E,gBAAD;AAFJ,GAAD,CAAb;AAKAnB,EAAAA,MAAM,GAAGS,QAAQ,CAACT,MAAT,CAAgBoB,OAAzB;AACAD,EAAAA,gBAAgB,GAAGV,QAAQ,CAACT,MAAT,CAAgBqB,YAAnC;;AAEA,SAAOrB,MAAP,EAAe;AACbR,IAAAA,IAAI,CAACwB,IAAL,CAAU;AACRhB,MAAAA,MADQ;AAERF,MAAAA,MAAM,EAAExD,QAAQ,CAAC6E,gBAAD;AAFR,KAAV;AAKAA,IAAAA,gBAAgB,GAAGnB,MAAM,CAACqB,YAA1B;AACArB,IAAAA,MAAM,GAAGA,MAAM,CAACoB,OAAhB;AACD;;AAED5B,EAAAA,IAAI,CAAC8B,OAAL;AAEA,SAAO9B,IAAP;AACD,CAxBD;;AA0BA+B,MAAM,CAACC,OAAP,GAAiB3E,OAAjB","sourcesContent":["'use strict'\n\nconst {\n  DAGLink,\n  DAGNode\n} = require('ipld-dag-pb')\nconst CID = require('cids')\nconst log = require('debug')('ipfs:mfs:core:utils:add-link')\nconst UnixFS = require('ipfs-unixfs')\nconst DirSharded = require('ipfs-unixfs-importer/src/dir-sharded')\nconst {\n  updateHamtDirectory,\n  recreateHamtLevel,\n  createShard,\n  toPrefix,\n  addLinksToHamtBucket\n} = require('./hamt-utils')\nconst errCode = require('err-code')\nconst mc = require('multicodec')\nconst mh = require('multihashing-async').multihash\nconst last = require('it-last')\n\nconst addLink = async (context, options) => {\n  if (!options.parentCid && !options.parent) {\n    throw errCode(new Error('No parent node or CID passed to addLink'), 'EINVALIDPARENT')\n  }\n\n  if (options.parentCid && !CID.isCID(options.parentCid)) {\n    throw errCode(new Error('Invalid CID passed to addLink'), 'EINVALIDPARENTCID')\n  }\n\n  if (!options.parent) {\n    log(`Loading parent node ${options.parentCid}`)\n\n    options.parent = await context.ipld.get(options.parentCid)\n  }\n\n  if (!options.cid) {\n    throw errCode(new Error('No child cid passed to addLink'), 'EINVALIDCHILDCID')\n  }\n\n  if (!options.name) {\n    throw errCode(new Error('No child name passed to addLink'), 'EINVALIDCHILDNAME')\n  }\n\n  if (!CID.isCID(options.cid)) {\n    options.cid = new CID(options.cid)\n  }\n\n  if (!options.size && options.size !== 0) {\n    throw errCode(new Error('No child size passed to addLink'), 'EINVALIDCHILDSIZE')\n  }\n\n  const meta = UnixFS.unmarshal(options.parent.Data)\n\n  if (meta.type === 'hamt-sharded-directory') {\n    log('Adding link to sharded directory')\n\n    return addToShardedDirectory(context, options)\n  }\n\n  if (options.parent.Links.length >= options.shardSplitThreshold) {\n    log('Converting directory to sharded directory')\n\n    return convertToShardedDirectory(context, {\n      ...options,\n      mtime: meta.mtime,\n      mode: meta.mode\n    })\n  }\n\n  log(`Adding ${options.name} (${options.cid}) to regular directory`)\n\n  return addToDirectory(context, options)\n}\n\nconst convertToShardedDirectory = async (context, options) => {\n  const result = await createShard(context, options.parent.Links.map(link => ({\n    name: link.Name,\n    size: link.Tsize,\n    cid: link.Hash\n  })).concat({\n    name: options.name,\n    size: options.size,\n    cid: options.cid\n  }), options)\n\n  log(`Converted directory to sharded directory ${result.cid}`)\n\n  return result\n}\n\nconst addToDirectory = async (context, options) => {\n  options.parent.rmLink(options.name)\n  options.parent.addLink(new DAGLink(options.name, options.size, options.cid))\n\n  const node = UnixFS.unmarshal(options.parent.Data)\n\n  if (node.mtime) {\n    // Update mtime if previously set\n    node.mtime = new Date()\n\n    options.parent = new DAGNode(node.marshal(), options.parent.Links)\n  }\n\n  const hashAlg = mh.names[options.hashAlg]\n\n  // Persist the new parent DAGNode\n  const cid = await context.ipld.put(options.parent, mc.DAG_PB, {\n    cidVersion: options.cidVersion,\n    hashAlg,\n    onlyHash: !options.flush\n  })\n\n  return {\n    node: options.parent,\n    cid,\n    size: options.parent.size\n  }\n}\n\nconst addToShardedDirectory = async (context, options) => {\n  const {\n    shard, path\n  } = await addFileToShardedDirectory(context, options)\n\n  const result = await last(shard.flush('', context.block))\n  const node = await context.ipld.get(result.cid)\n\n  // we have written out the shard, but only one sub-shard will have been written so replace it in the original shard\n  const oldLink = options.parent.Links\n    .find(link => link.Name.substring(0, 2) === path[0].prefix)\n\n  const newLink = node.Links\n    .find(link => link.Name.substring(0, 2) === path[0].prefix)\n\n  if (oldLink) {\n    options.parent.rmLink(oldLink.Name)\n  }\n\n  options.parent.addLink(newLink)\n\n  return updateHamtDirectory(context, options.parent.Links, path[0].bucket, options)\n}\n\nconst addFileToShardedDirectory = async (context, options) => {\n  const file = {\n    name: options.name,\n    cid: options.cid,\n    size: options.size\n  }\n\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateHamtLevel(options.parent.Links)\n  const node = UnixFS.unmarshal(options.parent.Data)\n\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: null,\n    parentKey: null,\n    path: '',\n    dirty: true,\n    flat: false,\n    mode: node.mode\n  }, options)\n  shard._bucket = rootBucket\n\n  if (node.mtime) {\n    // update mtime if previously set\n    shard.mtime = new Date()\n  }\n\n  // load subshards until the bucket & position no longer changes\n  const position = await rootBucket._findNewBucketAndPos(file.name)\n  const path = toBucketPath(position)\n  path[0].node = options.parent\n  let index = 0\n\n  while (index < path.length) {\n    const segment = path[index]\n    index++\n    const node = segment.node\n\n    const link = node.Links\n      .find(link => link.Name.substring(0, 2) === segment.prefix)\n\n    if (!link) {\n      // prefix is new, file will be added to the current bucket\n      log(`Link ${segment.prefix}${file.name} will be added`)\n      index = path.length\n\n      break\n    }\n\n    if (link.Name === `${segment.prefix}${file.name}`) {\n      // file already existed, file will be added to the current bucket\n      log(`Link ${segment.prefix}${file.name} will be replaced`)\n      index = path.length\n\n      break\n    }\n\n    if (link.Name.length > 2) {\n      // another file had the same prefix, will be replaced with a subshard\n      log(`Link ${link.Name} ${link.Hash} will be replaced with a subshard`)\n      index = path.length\n\n      break\n    }\n\n    // load sub-shard\n    log(`Found subshard ${segment.prefix}`)\n    const subShard = await context.ipld.get(link.Hash)\n\n    // subshard hasn't been loaded, descend to the next level of the HAMT\n    if (!path[index]) {\n      log(`Loaded new subshard ${segment.prefix}`)\n      await recreateHamtLevel(subShard.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16))\n\n      const position = await rootBucket._findNewBucketAndPos(file.name)\n\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: subShard\n      })\n\n      break\n    }\n\n    const nextSegment = path[index]\n\n    // add next levels worth of links to bucket\n    await addLinksToHamtBucket(subShard.Links, nextSegment.bucket, rootBucket)\n\n    nextSegment.node = subShard\n  }\n\n  // finally add the new file into the shard\n  await shard._bucket.put(file.name, {\n    size: file.size,\n    cid: file.cid\n  })\n\n  return {\n    shard, path\n  }\n}\n\nconst toBucketPath = (position) => {\n  let bucket = position.bucket\n  let positionInBucket = position.pos\n  const path = [{\n    bucket,\n    prefix: toPrefix(positionInBucket)\n  }]\n\n  bucket = position.bucket._parent\n  positionInBucket = position.bucket._posAtParent\n\n  while (bucket) {\n    path.push({\n      bucket,\n      prefix: toPrefix(positionInBucket)\n    })\n\n    positionInBucket = bucket._posAtParent\n    bucket = bucket._parent\n  }\n\n  path.reverse()\n\n  return path\n}\n\nmodule.exports = addLink\n"]},"metadata":{},"sourceType":"script"}