{"ast":null,"code":"'use strict';\n\nvar _objectSpread = require(\"/Users/yenerkaraca/Documents/GitHub/eth-app/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/objectSpread2\");\n\nvar _regeneratorRuntime = require(\"/Users/yenerkaraca/Documents/GitHub/eth-app/node_modules/babel-preset-react-app/node_modules/@babel/runtime/regenerator\");\n\nvar _asyncToGenerator = require(\"/Users/yenerkaraca/Documents/GitHub/eth-app/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/asyncToGenerator\");\n\nvar _require = require('ipld-dag-pb'),\n    DAGNode = _require.DAGNode;\n\nvar Bucket = require('hamt-sharding/src/bucket');\n\nvar DirSharded = require('ipfs-unixfs-importer/src/dir-sharded');\n\nvar log = require('debug')('ipfs:mfs:core:utils:hamt-utils');\n\nvar UnixFS = require('ipfs-unixfs');\n\nvar mc = require('multicodec');\n\nvar mh = require('multihashing-async').multihash;\n\nvar last = require('it-last');\n\nvar _require2 = require('buffer'),\n    Buffer = _require2.Buffer;\n\nvar updateHamtDirectory = /*#__PURE__*/function () {\n  var _ref = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee(context, links, bucket, options) {\n    var data, node, dir, hashAlg, parent, cid;\n    return _regeneratorRuntime.wrap(function _callee$(_context) {\n      while (1) {\n        switch (_context.prev = _context.next) {\n          case 0:\n            // update parent with new bit field\n            data = Buffer.from(bucket._children.bitField().reverse());\n            node = UnixFS.unmarshal(options.parent.Data);\n            dir = new UnixFS({\n              type: 'hamt-sharded-directory',\n              data: data,\n              fanout: bucket.tableSize(),\n              hashType: DirSharded.hashFn.code,\n              mode: node.mode,\n              mtime: node.mtime\n            });\n            hashAlg = mh.names[options.hashAlg];\n            parent = new DAGNode(dir.marshal(), links);\n            _context.next = 7;\n            return context.ipld.put(parent, mc.DAG_PB, {\n              cidVersion: options.cidVersion,\n              hashAlg: hashAlg,\n              onlyHash: !options.flush\n            });\n\n          case 7:\n            cid = _context.sent;\n            return _context.abrupt(\"return\", {\n              node: parent,\n              cid: cid,\n              size: parent.size\n            });\n\n          case 9:\n          case \"end\":\n            return _context.stop();\n        }\n      }\n    }, _callee);\n  }));\n\n  return function updateHamtDirectory(_x, _x2, _x3, _x4) {\n    return _ref.apply(this, arguments);\n  };\n}();\n\nvar recreateHamtLevel = /*#__PURE__*/function () {\n  var _ref2 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2(links, rootBucket, parentBucket, positionAtParent) {\n    var bucket;\n    return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n      while (1) {\n        switch (_context2.prev = _context2.next) {\n          case 0:\n            // recreate this level of the HAMT\n            bucket = new Bucket({\n              hashFn: DirSharded.hashFn,\n              hash: parentBucket ? parentBucket._options.hash : undefined\n            }, parentBucket, positionAtParent);\n\n            if (parentBucket) {\n              parentBucket._putObjectAt(positionAtParent, bucket);\n            }\n\n            _context2.next = 4;\n            return addLinksToHamtBucket(links, bucket, rootBucket);\n\n          case 4:\n            return _context2.abrupt(\"return\", bucket);\n\n          case 5:\n          case \"end\":\n            return _context2.stop();\n        }\n      }\n    }, _callee2);\n  }));\n\n  return function recreateHamtLevel(_x5, _x6, _x7, _x8) {\n    return _ref2.apply(this, arguments);\n  };\n}();\n\nvar addLinksToHamtBucket = /*#__PURE__*/function () {\n  var _ref3 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee3(links, bucket, rootBucket) {\n    return _regeneratorRuntime.wrap(function _callee3$(_context3) {\n      while (1) {\n        switch (_context3.prev = _context3.next) {\n          case 0:\n            _context3.next = 2;\n            return Promise.all(links.map(function (link) {\n              if (link.Name.length === 2) {\n                var pos = parseInt(link.Name, 16);\n\n                bucket._putObjectAt(pos, new Bucket({\n                  hashFn: DirSharded.hashFn\n                }, bucket, pos));\n\n                return Promise.resolve();\n              }\n\n              return (rootBucket || bucket).put(link.Name.substring(2), {\n                size: link.Tsize,\n                cid: link.Hash\n              });\n            }));\n\n          case 2:\n          case \"end\":\n            return _context3.stop();\n        }\n      }\n    }, _callee3);\n  }));\n\n  return function addLinksToHamtBucket(_x9, _x10, _x11) {\n    return _ref3.apply(this, arguments);\n  };\n}();\n\nvar toPrefix = function toPrefix(position) {\n  return position.toString('16').toUpperCase().padStart(2, '0').substring(0, 2);\n};\n\nvar generatePath = /*#__PURE__*/function () {\n  var _ref4 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee4(context, fileName, rootNode) {\n    var rootBucket, position, path, currentBucket, _loop, i, _ret;\n\n    return _regeneratorRuntime.wrap(function _callee4$(_context5) {\n      while (1) {\n        switch (_context5.prev = _context5.next) {\n          case 0:\n            _context5.next = 2;\n            return recreateHamtLevel(rootNode.Links, null, null, null);\n\n          case 2:\n            rootBucket = _context5.sent;\n            _context5.next = 5;\n            return rootBucket._findNewBucketAndPos(fileName);\n\n          case 5:\n            position = _context5.sent;\n            // the path to the root bucket\n            path = [{\n              bucket: position.bucket,\n              prefix: toPrefix(position.pos)\n            }];\n            currentBucket = position.bucket;\n\n            while (currentBucket !== rootBucket) {\n              path.push({\n                bucket: currentBucket,\n                prefix: toPrefix(currentBucket._posAtParent)\n              });\n              currentBucket = currentBucket._parent;\n            }\n\n            path.reverse();\n            path[0].node = rootNode; // load DAGNode for each path segment\n\n            _loop = /*#__PURE__*/_regeneratorRuntime.mark(function _loop(i) {\n              var segment, link, node, _position, nextSegment;\n\n              return _regeneratorRuntime.wrap(function _loop$(_context4) {\n                while (1) {\n                  switch (_context4.prev = _context4.next) {\n                    case 0:\n                      segment = path[i]; // find prefix in links\n\n                      link = segment.node.Links.filter(function (link) {\n                        return link.Name.substring(0, 2) === segment.prefix;\n                      }).pop(); // entry was not in shard\n\n                      if (link) {\n                        _context4.next = 5;\n                        break;\n                      }\n\n                      // reached bottom of tree, file will be added to the current bucket\n                      log(\"Link \".concat(segment.prefix).concat(fileName, \" will be added\")); // return path\n\n                      return _context4.abrupt(\"return\", \"continue\");\n\n                    case 5:\n                      if (!(link.Name === \"\".concat(segment.prefix).concat(fileName))) {\n                        _context4.next = 8;\n                        break;\n                      }\n\n                      log(\"Link \".concat(segment.prefix).concat(fileName, \" will be replaced\")); // file already existed, file will be added to the current bucket\n                      // return path\n\n                      return _context4.abrupt(\"return\", \"continue\");\n\n                    case 8:\n                      // found subshard\n                      log(\"Found subshard \".concat(segment.prefix));\n                      _context4.next = 11;\n                      return context.ipld.get(link.Hash);\n\n                    case 11:\n                      node = _context4.sent;\n\n                      if (path[i + 1]) {\n                        _context4.next = 21;\n                        break;\n                      }\n\n                      log(\"Loaded new subshard \".concat(segment.prefix));\n                      _context4.next = 16;\n                      return recreateHamtLevel(node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n\n                    case 16:\n                      _context4.next = 18;\n                      return rootBucket._findNewBucketAndPos(fileName);\n\n                    case 18:\n                      _position = _context4.sent;\n                      // i--\n                      path.push({\n                        bucket: _position.bucket,\n                        prefix: toPrefix(_position.pos),\n                        node: node\n                      });\n                      return _context4.abrupt(\"return\", \"continue\");\n\n                    case 21:\n                      nextSegment = path[i + 1]; // add intermediate links to bucket\n\n                      _context4.next = 24;\n                      return addLinksToHamtBucket(node.Links, nextSegment.bucket, rootBucket);\n\n                    case 24:\n                      nextSegment.node = node;\n\n                    case 25:\n                    case \"end\":\n                      return _context4.stop();\n                  }\n                }\n              }, _loop);\n            });\n            i = 0;\n\n          case 13:\n            if (!(i < path.length)) {\n              _context5.next = 21;\n              break;\n            }\n\n            return _context5.delegateYield(_loop(i), \"t0\", 15);\n\n          case 15:\n            _ret = _context5.t0;\n\n            if (!(_ret === \"continue\")) {\n              _context5.next = 18;\n              break;\n            }\n\n            return _context5.abrupt(\"continue\", 18);\n\n          case 18:\n            i++;\n            _context5.next = 13;\n            break;\n\n          case 21:\n            _context5.next = 23;\n            return rootBucket.put(fileName, true);\n\n          case 23:\n            path.reverse();\n            return _context5.abrupt(\"return\", {\n              rootBucket: rootBucket,\n              path: path\n            });\n\n          case 25:\n          case \"end\":\n            return _context5.stop();\n        }\n      }\n    }, _callee4);\n  }));\n\n  return function generatePath(_x12, _x13, _x14) {\n    return _ref4.apply(this, arguments);\n  };\n}();\n\nvar createShard = /*#__PURE__*/function () {\n  var _ref5 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee5(context, contents, options) {\n    var shard, i;\n    return _regeneratorRuntime.wrap(function _callee5$(_context6) {\n      while (1) {\n        switch (_context6.prev = _context6.next) {\n          case 0:\n            shard = new DirSharded({\n              root: true,\n              dir: true,\n              parent: null,\n              parentKey: null,\n              path: '',\n              dirty: true,\n              flat: false,\n              mtime: options.mtime,\n              mode: options.mode\n            }, _objectSpread(_objectSpread({}, options), {}, {\n              codec: 'dag-pb'\n            }));\n            i = 0;\n\n          case 2:\n            if (!(i < contents.length)) {\n              _context6.next = 8;\n              break;\n            }\n\n            _context6.next = 5;\n            return shard._bucket.put(contents[i].name, {\n              size: contents[i].size,\n              cid: contents[i].cid\n            });\n\n          case 5:\n            i++;\n            _context6.next = 2;\n            break;\n\n          case 8:\n            return _context6.abrupt(\"return\", last(shard.flush('', context.block, null)));\n\n          case 9:\n          case \"end\":\n            return _context6.stop();\n        }\n      }\n    }, _callee5);\n  }));\n\n  return function createShard(_x15, _x16, _x17) {\n    return _ref5.apply(this, arguments);\n  };\n}();\n\nmodule.exports = {\n  generatePath: generatePath,\n  updateHamtDirectory: updateHamtDirectory,\n  recreateHamtLevel: recreateHamtLevel,\n  addLinksToHamtBucket: addLinksToHamtBucket,\n  toPrefix: toPrefix,\n  createShard: createShard\n};","map":{"version":3,"sources":["/Users/yenerkaraca/Documents/GitHub/eth-app/node_modules/ipfs/src/core/components/files/utils/hamt-utils.js"],"names":["require","DAGNode","Bucket","DirSharded","log","UnixFS","mc","mh","multihash","last","Buffer","updateHamtDirectory","context","links","bucket","options","data","from","_children","bitField","reverse","node","unmarshal","parent","Data","dir","type","fanout","tableSize","hashType","hashFn","code","mode","mtime","hashAlg","names","marshal","ipld","put","DAG_PB","cidVersion","onlyHash","flush","cid","size","recreateHamtLevel","rootBucket","parentBucket","positionAtParent","hash","_options","undefined","_putObjectAt","addLinksToHamtBucket","Promise","all","map","link","Name","length","pos","parseInt","resolve","substring","Tsize","Hash","toPrefix","position","toString","toUpperCase","padStart","generatePath","fileName","rootNode","Links","_findNewBucketAndPos","path","prefix","currentBucket","push","_posAtParent","_parent","i","segment","filter","pop","get","nextSegment","createShard","contents","shard","root","parentKey","dirty","flat","codec","_bucket","name","block","module","exports"],"mappings":"AAAA;;;;;;;;eAIIA,OAAO,CAAC,aAAD,C;IADTC,O,YAAAA,O;;AAEF,IAAMC,MAAM,GAAGF,OAAO,CAAC,0BAAD,CAAtB;;AACA,IAAMG,UAAU,GAAGH,OAAO,CAAC,sCAAD,CAA1B;;AACA,IAAMI,GAAG,GAAGJ,OAAO,CAAC,OAAD,CAAP,CAAiB,gCAAjB,CAAZ;;AACA,IAAMK,MAAM,GAAGL,OAAO,CAAC,aAAD,CAAtB;;AACA,IAAMM,EAAE,GAAGN,OAAO,CAAC,YAAD,CAAlB;;AACA,IAAMO,EAAE,GAAGP,OAAO,CAAC,oBAAD,CAAP,CAA8BQ,SAAzC;;AACA,IAAMC,IAAI,GAAGT,OAAO,CAAC,SAAD,CAApB;;gBACmBA,OAAO,CAAC,QAAD,C;IAAlBU,M,aAAAA,M;;AAER,IAAMC,mBAAmB;AAAA,sEAAG,iBAAOC,OAAP,EAAgBC,KAAhB,EAAuBC,MAAvB,EAA+BC,OAA/B;AAAA;AAAA;AAAA;AAAA;AAAA;AAC1B;AACMC,YAAAA,IAFoB,GAEbN,MAAM,CAACO,IAAP,CAAYH,MAAM,CAACI,SAAP,CAAiBC,QAAjB,GAA4BC,OAA5B,EAAZ,CAFa;AAGpBC,YAAAA,IAHoB,GAGbhB,MAAM,CAACiB,SAAP,CAAiBP,OAAO,CAACQ,MAAR,CAAeC,IAAhC,CAHa;AAIpBC,YAAAA,GAJoB,GAId,IAAIpB,MAAJ,CAAW;AACrBqB,cAAAA,IAAI,EAAE,wBADe;AAErBV,cAAAA,IAAI,EAAJA,IAFqB;AAGrBW,cAAAA,MAAM,EAAEb,MAAM,CAACc,SAAP,EAHa;AAIrBC,cAAAA,QAAQ,EAAE1B,UAAU,CAAC2B,MAAX,CAAkBC,IAJP;AAKrBC,cAAAA,IAAI,EAAEX,IAAI,CAACW,IALU;AAMrBC,cAAAA,KAAK,EAAEZ,IAAI,CAACY;AANS,aAAX,CAJc;AAapBC,YAAAA,OAboB,GAaV3B,EAAE,CAAC4B,KAAH,CAASpB,OAAO,CAACmB,OAAjB,CAbU;AAcpBX,YAAAA,MAdoB,GAcX,IAAItB,OAAJ,CAAYwB,GAAG,CAACW,OAAJ,EAAZ,EAA2BvB,KAA3B,CAdW;AAAA;AAAA,mBAeRD,OAAO,CAACyB,IAAR,CAAaC,GAAb,CAAiBf,MAAjB,EAAyBjB,EAAE,CAACiC,MAA5B,EAAoC;AACpDC,cAAAA,UAAU,EAAEzB,OAAO,CAACyB,UADgC;AAEpDN,cAAAA,OAAO,EAAPA,OAFoD;AAGpDO,cAAAA,QAAQ,EAAE,CAAC1B,OAAO,CAAC2B;AAHiC,aAApC,CAfQ;;AAAA;AAepBC,YAAAA,GAfoB;AAAA,6CAqBnB;AACLtB,cAAAA,IAAI,EAAEE,MADD;AAELoB,cAAAA,GAAG,EAAHA,GAFK;AAGLC,cAAAA,IAAI,EAAErB,MAAM,CAACqB;AAHR,aArBmB;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,GAAH;;AAAA,kBAAnBjC,mBAAmB;AAAA;AAAA;AAAA,GAAzB;;AA4BA,IAAMkC,iBAAiB;AAAA,uEAAG,kBAAOhC,KAAP,EAAciC,UAAd,EAA0BC,YAA1B,EAAwCC,gBAAxC;AAAA;AAAA;AAAA;AAAA;AAAA;AACxB;AACMlC,YAAAA,MAFkB,GAET,IAAIZ,MAAJ,CAAW;AACxB4B,cAAAA,MAAM,EAAE3B,UAAU,CAAC2B,MADK;AAExBmB,cAAAA,IAAI,EAAEF,YAAY,GAAGA,YAAY,CAACG,QAAb,CAAsBD,IAAzB,GAAgCE;AAF1B,aAAX,EAGZJ,YAHY,EAGEC,gBAHF,CAFS;;AAOxB,gBAAID,YAAJ,EAAkB;AAChBA,cAAAA,YAAY,CAACK,YAAb,CAA0BJ,gBAA1B,EAA4ClC,MAA5C;AACD;;AATuB;AAAA,mBAWlBuC,oBAAoB,CAACxC,KAAD,EAAQC,MAAR,EAAgBgC,UAAhB,CAXF;;AAAA;AAAA,8CAajBhC,MAbiB;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,GAAH;;AAAA,kBAAjB+B,iBAAiB;AAAA;AAAA;AAAA,GAAvB;;AAgBA,IAAMQ,oBAAoB;AAAA,uEAAG,kBAAOxC,KAAP,EAAcC,MAAd,EAAsBgC,UAAtB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mBACrBQ,OAAO,CAACC,GAAR,CACJ1C,KAAK,CAAC2C,GAAN,CAAU,UAAAC,IAAI,EAAI;AAChB,kBAAIA,IAAI,CAACC,IAAL,CAAUC,MAAV,KAAqB,CAAzB,EAA4B;AAC1B,oBAAMC,GAAG,GAAGC,QAAQ,CAACJ,IAAI,CAACC,IAAN,EAAY,EAAZ,CAApB;;AAEA5C,gBAAAA,MAAM,CAACsC,YAAP,CAAoBQ,GAApB,EAAyB,IAAI1D,MAAJ,CAAW;AAClC4B,kBAAAA,MAAM,EAAE3B,UAAU,CAAC2B;AADe,iBAAX,EAEtBhB,MAFsB,EAEd8C,GAFc,CAAzB;;AAIA,uBAAON,OAAO,CAACQ,OAAR,EAAP;AACD;;AAED,qBAAO,CAAChB,UAAU,IAAIhC,MAAf,EAAuBwB,GAAvB,CAA2BmB,IAAI,CAACC,IAAL,CAAUK,SAAV,CAAoB,CAApB,CAA3B,EAAmD;AACxDnB,gBAAAA,IAAI,EAAEa,IAAI,CAACO,KAD6C;AAExDrB,gBAAAA,GAAG,EAAEc,IAAI,CAACQ;AAF8C,eAAnD,CAAP;AAID,aAfD,CADI,CADqB;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,GAAH;;AAAA,kBAApBZ,oBAAoB;AAAA;AAAA;AAAA,GAA1B;;AAqBA,IAAMa,QAAQ,GAAG,SAAXA,QAAW,CAACC,QAAD,EAAc;AAC7B,SAAOA,QAAQ,CACZC,QADI,CACK,IADL,EAEJC,WAFI,GAGJC,QAHI,CAGK,CAHL,EAGQ,GAHR,EAIJP,SAJI,CAIM,CAJN,EAIS,CAJT,CAAP;AAKD,CAND;;AAQA,IAAMQ,YAAY;AAAA,uEAAG,kBAAO3D,OAAP,EAAgB4D,QAAhB,EAA0BC,QAA1B;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mBAEM5B,iBAAiB,CAAC4B,QAAQ,CAACC,KAAV,EAAiB,IAAjB,EAAuB,IAAvB,EAA6B,IAA7B,CAFvB;;AAAA;AAEb5B,YAAAA,UAFa;AAAA;AAAA,mBAGIA,UAAU,CAAC6B,oBAAX,CAAgCH,QAAhC,CAHJ;;AAAA;AAGbL,YAAAA,QAHa;AAKnB;AACMS,YAAAA,IANa,GAMN,CAAC;AACZ9D,cAAAA,MAAM,EAAEqD,QAAQ,CAACrD,MADL;AAEZ+D,cAAAA,MAAM,EAAEX,QAAQ,CAACC,QAAQ,CAACP,GAAV;AAFJ,aAAD,CANM;AAUfkB,YAAAA,aAVe,GAUCX,QAAQ,CAACrD,MAVV;;AAYnB,mBAAOgE,aAAa,KAAKhC,UAAzB,EAAqC;AACnC8B,cAAAA,IAAI,CAACG,IAAL,CAAU;AACRjE,gBAAAA,MAAM,EAAEgE,aADA;AAERD,gBAAAA,MAAM,EAAEX,QAAQ,CAACY,aAAa,CAACE,YAAf;AAFR,eAAV;AAKAF,cAAAA,aAAa,GAAGA,aAAa,CAACG,OAA9B;AACD;;AAEDL,YAAAA,IAAI,CAACxD,OAAL;AACAwD,YAAAA,IAAI,CAAC,CAAD,CAAJ,CAAQvD,IAAR,GAAeoD,QAAf,CAtBmB,CAwBnB;;AAxBmB,yEAyBVS,CAzBU;AAAA;;AAAA;AAAA;AAAA;AAAA;AA0BXC,sBAAAA,OA1BW,GA0BDP,IAAI,CAACM,CAAD,CA1BH,EA4BjB;;AACMzB,sBAAAA,IA7BW,GA6BJ0B,OAAO,CAAC9D,IAAR,CAAaqD,KAAb,CACVU,MADU,CACH,UAAA3B,IAAI;AAAA,+BAAIA,IAAI,CAACC,IAAL,CAAUK,SAAV,CAAoB,CAApB,EAAuB,CAAvB,MAA8BoB,OAAO,CAACN,MAA1C;AAAA,uBADD,EAEVQ,GAFU,EA7BI,EAiCjB;;AAjCiB,0BAkCZ5B,IAlCY;AAAA;AAAA;AAAA;;AAmCf;AACArD,sBAAAA,GAAG,gBAAS+E,OAAO,CAACN,MAAjB,SAA0BL,QAA1B,oBAAH,CApCe,CAqCf;;AArCe;;AAAA;AAAA,4BA0Cbf,IAAI,CAACC,IAAL,eAAiByB,OAAO,CAACN,MAAzB,SAAkCL,QAAlC,CA1Ca;AAAA;AAAA;AAAA;;AA2CfpE,sBAAAA,GAAG,gBAAS+E,OAAO,CAACN,MAAjB,SAA0BL,QAA1B,uBAAH,CA3Ce,CA4Cf;AACA;;AA7Ce;;AAAA;AAiDjB;AACApE,sBAAAA,GAAG,0BAAmB+E,OAAO,CAACN,MAA3B,EAAH;AAlDiB;AAAA,6BAmDEjE,OAAO,CAACyB,IAAR,CAAaiD,GAAb,CAAiB7B,IAAI,CAACQ,IAAtB,CAnDF;;AAAA;AAmDX5C,sBAAAA,IAnDW;;AAAA,0BAsDZuD,IAAI,CAACM,CAAC,GAAG,CAAL,CAtDQ;AAAA;AAAA;AAAA;;AAuDf9E,sBAAAA,GAAG,+BAAwB+E,OAAO,CAACN,MAAhC,EAAH;AAvDe;AAAA,6BAyDThC,iBAAiB,CAACxB,IAAI,CAACqD,KAAN,EAAa5B,UAAb,EAAyBqC,OAAO,CAACrE,MAAjC,EAAyC+C,QAAQ,CAACsB,OAAO,CAACN,MAAT,EAAiB,EAAjB,CAAjD,CAzDR;;AAAA;AAAA;AAAA,6BA0DQ/B,UAAU,CAAC6B,oBAAX,CAAgCH,QAAhC,CA1DR;;AAAA;AA0DTL,sBAAAA,SA1DS;AA4Df;AACAS,sBAAAA,IAAI,CAACG,IAAL,CAAU;AACRjE,wBAAAA,MAAM,EAAEqD,SAAQ,CAACrD,MADT;AAER+D,wBAAAA,MAAM,EAAEX,QAAQ,CAACC,SAAQ,CAACP,GAAV,CAFR;AAGRvC,wBAAAA,IAAI,EAAEA;AAHE,uBAAV;AA7De;;AAAA;AAsEXkE,sBAAAA,WAtEW,GAsEGX,IAAI,CAACM,CAAC,GAAG,CAAL,CAtEP,EAwEjB;;AAxEiB;AAAA,6BAyEX7B,oBAAoB,CAAChC,IAAI,CAACqD,KAAN,EAAaa,WAAW,CAACzE,MAAzB,EAAiCgC,UAAjC,CAzET;;AAAA;AA2EjByC,sBAAAA,WAAW,CAAClE,IAAZ,GAAmBA,IAAnB;;AA3EiB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAyBV6D,YAAAA,CAzBU,GAyBN,CAzBM;;AAAA;AAAA,kBAyBHA,CAAC,GAAGN,IAAI,CAACjB,MAzBN;AAAA;AAAA;AAAA;;AAAA,iDAyBVuB,CAzBU;;AAAA;AAAA;;AAAA;AAAA;AAAA;AAAA;;AAAA;;AAAA;AAyBcA,YAAAA,CAAC,EAzBf;AAAA;AAAA;;AAAA;AAAA;AAAA,mBA8EbpC,UAAU,CAACR,GAAX,CAAekC,QAAf,EAAyB,IAAzB,CA9Ea;;AAAA;AAgFnBI,YAAAA,IAAI,CAACxD,OAAL;AAhFmB,8CAkFZ;AACL0B,cAAAA,UAAU,EAAVA,UADK;AAEL8B,cAAAA,IAAI,EAAJA;AAFK,aAlFY;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,GAAH;;AAAA,kBAAZL,YAAY;AAAA;AAAA;AAAA,GAAlB;;AAwFA,IAAMiB,WAAW;AAAA,uEAAG,kBAAO5E,OAAP,EAAgB6E,QAAhB,EAA0B1E,OAA1B;AAAA;AAAA;AAAA;AAAA;AAAA;AACZ2E,YAAAA,KADY,GACJ,IAAIvF,UAAJ,CAAe;AAC3BwF,cAAAA,IAAI,EAAE,IADqB;AAE3BlE,cAAAA,GAAG,EAAE,IAFsB;AAG3BF,cAAAA,MAAM,EAAE,IAHmB;AAI3BqE,cAAAA,SAAS,EAAE,IAJgB;AAK3BhB,cAAAA,IAAI,EAAE,EALqB;AAM3BiB,cAAAA,KAAK,EAAE,IANoB;AAO3BC,cAAAA,IAAI,EAAE,KAPqB;AAQ3B7D,cAAAA,KAAK,EAAElB,OAAO,CAACkB,KARY;AAS3BD,cAAAA,IAAI,EAAEjB,OAAO,CAACiB;AATa,aAAf,kCAWTjB,OAXS;AAYZgF,cAAAA,KAAK,EAAE;AAZK,eADI;AAgBTb,YAAAA,CAhBS,GAgBL,CAhBK;;AAAA;AAAA,kBAgBFA,CAAC,GAAGO,QAAQ,CAAC9B,MAhBX;AAAA;AAAA;AAAA;;AAAA;AAAA,mBAiBV+B,KAAK,CAACM,OAAN,CAAc1D,GAAd,CAAkBmD,QAAQ,CAACP,CAAD,CAAR,CAAYe,IAA9B,EAAoC;AACxCrD,cAAAA,IAAI,EAAE6C,QAAQ,CAACP,CAAD,CAAR,CAAYtC,IADsB;AAExCD,cAAAA,GAAG,EAAE8C,QAAQ,CAACP,CAAD,CAAR,CAAYvC;AAFuB,aAApC,CAjBU;;AAAA;AAgBmBuC,YAAAA,CAAC,EAhBpB;AAAA;AAAA;;AAAA;AAAA,8CAuBXzE,IAAI,CAACiF,KAAK,CAAChD,KAAN,CAAY,EAAZ,EAAgB9B,OAAO,CAACsF,KAAxB,EAA+B,IAA/B,CAAD,CAvBO;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,GAAH;;AAAA,kBAAXV,WAAW;AAAA;AAAA;AAAA,GAAjB;;AA0BAW,MAAM,CAACC,OAAP,GAAiB;AACf7B,EAAAA,YAAY,EAAZA,YADe;AAEf5D,EAAAA,mBAAmB,EAAnBA,mBAFe;AAGfkC,EAAAA,iBAAiB,EAAjBA,iBAHe;AAIfQ,EAAAA,oBAAoB,EAApBA,oBAJe;AAKfa,EAAAA,QAAQ,EAARA,QALe;AAMfsB,EAAAA,WAAW,EAAXA;AANe,CAAjB","sourcesContent":["'use strict'\n\nconst {\n  DAGNode\n} = require('ipld-dag-pb')\nconst Bucket = require('hamt-sharding/src/bucket')\nconst DirSharded = require('ipfs-unixfs-importer/src/dir-sharded')\nconst log = require('debug')('ipfs:mfs:core:utils:hamt-utils')\nconst UnixFS = require('ipfs-unixfs')\nconst mc = require('multicodec')\nconst mh = require('multihashing-async').multihash\nconst last = require('it-last')\nconst { Buffer } = require('buffer')\n\nconst updateHamtDirectory = async (context, links, bucket, options) => {\n  // update parent with new bit field\n  const data = Buffer.from(bucket._children.bitField().reverse())\n  const node = UnixFS.unmarshal(options.parent.Data)\n  const dir = new UnixFS({\n    type: 'hamt-sharded-directory',\n    data,\n    fanout: bucket.tableSize(),\n    hashType: DirSharded.hashFn.code,\n    mode: node.mode,\n    mtime: node.mtime\n  })\n\n  const hashAlg = mh.names[options.hashAlg]\n  const parent = new DAGNode(dir.marshal(), links)\n  const cid = await context.ipld.put(parent, mc.DAG_PB, {\n    cidVersion: options.cidVersion,\n    hashAlg,\n    onlyHash: !options.flush\n  })\n\n  return {\n    node: parent,\n    cid,\n    size: parent.size\n  }\n}\n\nconst recreateHamtLevel = async (links, rootBucket, parentBucket, positionAtParent) => {\n  // recreate this level of the HAMT\n  const bucket = new Bucket({\n    hashFn: DirSharded.hashFn,\n    hash: parentBucket ? parentBucket._options.hash : undefined\n  }, parentBucket, positionAtParent)\n\n  if (parentBucket) {\n    parentBucket._putObjectAt(positionAtParent, bucket)\n  }\n\n  await addLinksToHamtBucket(links, bucket, rootBucket)\n\n  return bucket\n}\n\nconst addLinksToHamtBucket = async (links, bucket, rootBucket) => {\n  await Promise.all(\n    links.map(link => {\n      if (link.Name.length === 2) {\n        const pos = parseInt(link.Name, 16)\n\n        bucket._putObjectAt(pos, new Bucket({\n          hashFn: DirSharded.hashFn\n        }, bucket, pos))\n\n        return Promise.resolve()\n      }\n\n      return (rootBucket || bucket).put(link.Name.substring(2), {\n        size: link.Tsize,\n        cid: link.Hash\n      })\n    })\n  )\n}\n\nconst toPrefix = (position) => {\n  return position\n    .toString('16')\n    .toUpperCase()\n    .padStart(2, '0')\n    .substring(0, 2)\n}\n\nconst generatePath = async (context, fileName, rootNode) => {\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateHamtLevel(rootNode.Links, null, null, null)\n  const position = await rootBucket._findNewBucketAndPos(fileName)\n\n  // the path to the root bucket\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }]\n  let currentBucket = position.bucket\n\n  while (currentBucket !== rootBucket) {\n    path.push({\n      bucket: currentBucket,\n      prefix: toPrefix(currentBucket._posAtParent)\n    })\n\n    currentBucket = currentBucket._parent\n  }\n\n  path.reverse()\n  path[0].node = rootNode\n\n  // load DAGNode for each path segment\n  for (let i = 0; i < path.length; i++) {\n    const segment = path[i]\n\n    // find prefix in links\n    const link = segment.node.Links\n      .filter(link => link.Name.substring(0, 2) === segment.prefix)\n      .pop()\n\n    // entry was not in shard\n    if (!link) {\n      // reached bottom of tree, file will be added to the current bucket\n      log(`Link ${segment.prefix}${fileName} will be added`)\n      // return path\n      continue\n    }\n\n    // found entry\n    if (link.Name === `${segment.prefix}${fileName}`) {\n      log(`Link ${segment.prefix}${fileName} will be replaced`)\n      // file already existed, file will be added to the current bucket\n      // return path\n      continue\n    }\n\n    // found subshard\n    log(`Found subshard ${segment.prefix}`)\n    const node = await context.ipld.get(link.Hash)\n\n    // subshard hasn't been loaded, descend to the next level of the HAMT\n    if (!path[i + 1]) {\n      log(`Loaded new subshard ${segment.prefix}`)\n\n      await recreateHamtLevel(node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16))\n      const position = await rootBucket._findNewBucketAndPos(fileName)\n\n      // i--\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: node\n      })\n\n      continue\n    }\n\n    const nextSegment = path[i + 1]\n\n    // add intermediate links to bucket\n    await addLinksToHamtBucket(node.Links, nextSegment.bucket, rootBucket)\n\n    nextSegment.node = node\n  }\n\n  await rootBucket.put(fileName, true)\n\n  path.reverse()\n\n  return {\n    rootBucket,\n    path\n  }\n}\n\nconst createShard = async (context, contents, options) => {\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: null,\n    parentKey: null,\n    path: '',\n    dirty: true,\n    flat: false,\n    mtime: options.mtime,\n    mode: options.mode\n  }, {\n    ...options,\n    codec: 'dag-pb'\n  })\n\n  for (let i = 0; i < contents.length; i++) {\n    await shard._bucket.put(contents[i].name, {\n      size: contents[i].size,\n      cid: contents[i].cid\n    })\n  }\n\n  return last(shard.flush('', context.block, null))\n}\n\nmodule.exports = {\n  generatePath,\n  updateHamtDirectory,\n  recreateHamtLevel,\n  addLinksToHamtBucket,\n  toPrefix,\n  createShard\n}\n"]},"metadata":{},"sourceType":"script"}