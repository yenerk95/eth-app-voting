{"ast":null,"code":"'use strict';\n\nconst {\n  DAGNode\n} = require('ipld-dag-pb');\n\nconst Bucket = require('hamt-sharding/src/bucket');\n\nconst DirSharded = require('ipfs-unixfs-importer/src/dir-sharded');\n\nconst log = require('debug')('ipfs:mfs:core:utils:hamt-utils');\n\nconst UnixFS = require('ipfs-unixfs');\n\nconst mc = require('multicodec');\n\nconst mh = require('multihashing-async').multihash;\n\nconst last = require('it-last');\n\nconst {\n  Buffer\n} = require('buffer');\n\nconst updateHamtDirectory = async (context, links, bucket, options) => {\n  // update parent with new bit field\n  const data = Buffer.from(bucket._children.bitField().reverse());\n  const node = UnixFS.unmarshal(options.parent.Data);\n  const dir = new UnixFS({\n    type: 'hamt-sharded-directory',\n    data,\n    fanout: bucket.tableSize(),\n    hashType: DirSharded.hashFn.code,\n    mode: node.mode,\n    mtime: node.mtime\n  });\n  const hashAlg = mh.names[options.hashAlg];\n  const parent = new DAGNode(dir.marshal(), links);\n  const cid = await context.ipld.put(parent, mc.DAG_PB, {\n    cidVersion: options.cidVersion,\n    hashAlg,\n    onlyHash: !options.flush\n  });\n  return {\n    node: parent,\n    cid,\n    size: parent.size\n  };\n};\n\nconst recreateHamtLevel = async (links, rootBucket, parentBucket, positionAtParent) => {\n  // recreate this level of the HAMT\n  const bucket = new Bucket({\n    hashFn: DirSharded.hashFn,\n    hash: parentBucket ? parentBucket._options.hash : undefined\n  }, parentBucket, positionAtParent);\n\n  if (parentBucket) {\n    parentBucket._putObjectAt(positionAtParent, bucket);\n  }\n\n  await addLinksToHamtBucket(links, bucket, rootBucket);\n  return bucket;\n};\n\nconst addLinksToHamtBucket = async (links, bucket, rootBucket) => {\n  await Promise.all(links.map(link => {\n    if (link.Name.length === 2) {\n      const pos = parseInt(link.Name, 16);\n\n      bucket._putObjectAt(pos, new Bucket({\n        hashFn: DirSharded.hashFn\n      }, bucket, pos));\n\n      return Promise.resolve();\n    }\n\n    return (rootBucket || bucket).put(link.Name.substring(2), {\n      size: link.Tsize,\n      cid: link.Hash\n    });\n  }));\n};\n\nconst toPrefix = position => {\n  return position.toString('16').toUpperCase().padStart(2, '0').substring(0, 2);\n};\n\nconst generatePath = async (context, fileName, rootNode) => {\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateHamtLevel(rootNode.Links, null, null, null);\n  const position = await rootBucket._findNewBucketAndPos(fileName); // the path to the root bucket\n\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }];\n  let currentBucket = position.bucket;\n\n  while (currentBucket !== rootBucket) {\n    path.push({\n      bucket: currentBucket,\n      prefix: toPrefix(currentBucket._posAtParent)\n    });\n    currentBucket = currentBucket._parent;\n  }\n\n  path.reverse();\n  path[0].node = rootNode; // load DAGNode for each path segment\n\n  for (let i = 0; i < path.length; i++) {\n    const segment = path[i]; // find prefix in links\n\n    const link = segment.node.Links.filter(link => link.Name.substring(0, 2) === segment.prefix).pop(); // entry was not in shard\n\n    if (!link) {\n      // reached bottom of tree, file will be added to the current bucket\n      log(`Link ${segment.prefix}${fileName} will be added`); // return path\n\n      continue;\n    } // found entry\n\n\n    if (link.Name === `${segment.prefix}${fileName}`) {\n      log(`Link ${segment.prefix}${fileName} will be replaced`); // file already existed, file will be added to the current bucket\n      // return path\n\n      continue;\n    } // found subshard\n\n\n    log(`Found subshard ${segment.prefix}`);\n    const node = await context.ipld.get(link.Hash); // subshard hasn't been loaded, descend to the next level of the HAMT\n\n    if (!path[i + 1]) {\n      log(`Loaded new subshard ${segment.prefix}`);\n      await recreateHamtLevel(node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n      const position = await rootBucket._findNewBucketAndPos(fileName); // i--\n\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: node\n      });\n      continue;\n    }\n\n    const nextSegment = path[i + 1]; // add intermediate links to bucket\n\n    await addLinksToHamtBucket(node.Links, nextSegment.bucket, rootBucket);\n    nextSegment.node = node;\n  }\n\n  await rootBucket.put(fileName, true);\n  path.reverse();\n  return {\n    rootBucket,\n    path\n  };\n};\n\nconst createShard = async (context, contents, options) => {\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: null,\n    parentKey: null,\n    path: '',\n    dirty: true,\n    flat: false,\n    mtime: options.mtime,\n    mode: options.mode\n  }, { ...options,\n    codec: 'dag-pb'\n  });\n\n  for (let i = 0; i < contents.length; i++) {\n    await shard._bucket.put(contents[i].name, {\n      size: contents[i].size,\n      cid: contents[i].cid\n    });\n  }\n\n  return last(shard.flush('', context.block, null));\n};\n\nmodule.exports = {\n  generatePath,\n  updateHamtDirectory,\n  recreateHamtLevel,\n  addLinksToHamtBucket,\n  toPrefix,\n  createShard\n};","map":{"version":3,"sources":["/Users/yenerkaraca/Documents/GitHub/eth-app/node_modules/ipfs/src/core/components/files/utils/hamt-utils.js"],"names":["DAGNode","require","Bucket","DirSharded","log","UnixFS","mc","mh","multihash","last","Buffer","updateHamtDirectory","context","links","bucket","options","data","from","_children","bitField","reverse","node","unmarshal","parent","Data","dir","type","fanout","tableSize","hashType","hashFn","code","mode","mtime","hashAlg","names","marshal","cid","ipld","put","DAG_PB","cidVersion","onlyHash","flush","size","recreateHamtLevel","rootBucket","parentBucket","positionAtParent","hash","_options","undefined","_putObjectAt","addLinksToHamtBucket","Promise","all","map","link","Name","length","pos","parseInt","resolve","substring","Tsize","Hash","toPrefix","position","toString","toUpperCase","padStart","generatePath","fileName","rootNode","Links","_findNewBucketAndPos","path","prefix","currentBucket","push","_posAtParent","_parent","i","segment","filter","pop","get","nextSegment","createShard","contents","shard","root","parentKey","dirty","flat","codec","_bucket","name","block","module","exports"],"mappings":"AAAA;;AAEA,MAAM;AACJA,EAAAA;AADI,IAEFC,OAAO,CAAC,aAAD,CAFX;;AAGA,MAAMC,MAAM,GAAGD,OAAO,CAAC,0BAAD,CAAtB;;AACA,MAAME,UAAU,GAAGF,OAAO,CAAC,sCAAD,CAA1B;;AACA,MAAMG,GAAG,GAAGH,OAAO,CAAC,OAAD,CAAP,CAAiB,gCAAjB,CAAZ;;AACA,MAAMI,MAAM,GAAGJ,OAAO,CAAC,aAAD,CAAtB;;AACA,MAAMK,EAAE,GAAGL,OAAO,CAAC,YAAD,CAAlB;;AACA,MAAMM,EAAE,GAAGN,OAAO,CAAC,oBAAD,CAAP,CAA8BO,SAAzC;;AACA,MAAMC,IAAI,GAAGR,OAAO,CAAC,SAAD,CAApB;;AACA,MAAM;AAAES,EAAAA;AAAF,IAAaT,OAAO,CAAC,QAAD,CAA1B;;AAEA,MAAMU,mBAAmB,GAAG,OAAOC,OAAP,EAAgBC,KAAhB,EAAuBC,MAAvB,EAA+BC,OAA/B,KAA2C;AACrE;AACA,QAAMC,IAAI,GAAGN,MAAM,CAACO,IAAP,CAAYH,MAAM,CAACI,SAAP,CAAiBC,QAAjB,GAA4BC,OAA5B,EAAZ,CAAb;AACA,QAAMC,IAAI,GAAGhB,MAAM,CAACiB,SAAP,CAAiBP,OAAO,CAACQ,MAAR,CAAeC,IAAhC,CAAb;AACA,QAAMC,GAAG,GAAG,IAAIpB,MAAJ,CAAW;AACrBqB,IAAAA,IAAI,EAAE,wBADe;AAErBV,IAAAA,IAFqB;AAGrBW,IAAAA,MAAM,EAAEb,MAAM,CAACc,SAAP,EAHa;AAIrBC,IAAAA,QAAQ,EAAE1B,UAAU,CAAC2B,MAAX,CAAkBC,IAJP;AAKrBC,IAAAA,IAAI,EAAEX,IAAI,CAACW,IALU;AAMrBC,IAAAA,KAAK,EAAEZ,IAAI,CAACY;AANS,GAAX,CAAZ;AASA,QAAMC,OAAO,GAAG3B,EAAE,CAAC4B,KAAH,CAASpB,OAAO,CAACmB,OAAjB,CAAhB;AACA,QAAMX,MAAM,GAAG,IAAIvB,OAAJ,CAAYyB,GAAG,CAACW,OAAJ,EAAZ,EAA2BvB,KAA3B,CAAf;AACA,QAAMwB,GAAG,GAAG,MAAMzB,OAAO,CAAC0B,IAAR,CAAaC,GAAb,CAAiBhB,MAAjB,EAAyBjB,EAAE,CAACkC,MAA5B,EAAoC;AACpDC,IAAAA,UAAU,EAAE1B,OAAO,CAAC0B,UADgC;AAEpDP,IAAAA,OAFoD;AAGpDQ,IAAAA,QAAQ,EAAE,CAAC3B,OAAO,CAAC4B;AAHiC,GAApC,CAAlB;AAMA,SAAO;AACLtB,IAAAA,IAAI,EAAEE,MADD;AAELc,IAAAA,GAFK;AAGLO,IAAAA,IAAI,EAAErB,MAAM,CAACqB;AAHR,GAAP;AAKD,CA1BD;;AA4BA,MAAMC,iBAAiB,GAAG,OAAOhC,KAAP,EAAciC,UAAd,EAA0BC,YAA1B,EAAwCC,gBAAxC,KAA6D;AACrF;AACA,QAAMlC,MAAM,GAAG,IAAIZ,MAAJ,CAAW;AACxB4B,IAAAA,MAAM,EAAE3B,UAAU,CAAC2B,MADK;AAExBmB,IAAAA,IAAI,EAAEF,YAAY,GAAGA,YAAY,CAACG,QAAb,CAAsBD,IAAzB,GAAgCE;AAF1B,GAAX,EAGZJ,YAHY,EAGEC,gBAHF,CAAf;;AAKA,MAAID,YAAJ,EAAkB;AAChBA,IAAAA,YAAY,CAACK,YAAb,CAA0BJ,gBAA1B,EAA4ClC,MAA5C;AACD;;AAED,QAAMuC,oBAAoB,CAACxC,KAAD,EAAQC,MAAR,EAAgBgC,UAAhB,CAA1B;AAEA,SAAOhC,MAAP;AACD,CAdD;;AAgBA,MAAMuC,oBAAoB,GAAG,OAAOxC,KAAP,EAAcC,MAAd,EAAsBgC,UAAtB,KAAqC;AAChE,QAAMQ,OAAO,CAACC,GAAR,CACJ1C,KAAK,CAAC2C,GAAN,CAAUC,IAAI,IAAI;AAChB,QAAIA,IAAI,CAACC,IAAL,CAAUC,MAAV,KAAqB,CAAzB,EAA4B;AAC1B,YAAMC,GAAG,GAAGC,QAAQ,CAACJ,IAAI,CAACC,IAAN,EAAY,EAAZ,CAApB;;AAEA5C,MAAAA,MAAM,CAACsC,YAAP,CAAoBQ,GAApB,EAAyB,IAAI1D,MAAJ,CAAW;AAClC4B,QAAAA,MAAM,EAAE3B,UAAU,CAAC2B;AADe,OAAX,EAEtBhB,MAFsB,EAEd8C,GAFc,CAAzB;;AAIA,aAAON,OAAO,CAACQ,OAAR,EAAP;AACD;;AAED,WAAO,CAAChB,UAAU,IAAIhC,MAAf,EAAuByB,GAAvB,CAA2BkB,IAAI,CAACC,IAAL,CAAUK,SAAV,CAAoB,CAApB,CAA3B,EAAmD;AACxDnB,MAAAA,IAAI,EAAEa,IAAI,CAACO,KAD6C;AAExD3B,MAAAA,GAAG,EAAEoB,IAAI,CAACQ;AAF8C,KAAnD,CAAP;AAID,GAfD,CADI,CAAN;AAkBD,CAnBD;;AAqBA,MAAMC,QAAQ,GAAIC,QAAD,IAAc;AAC7B,SAAOA,QAAQ,CACZC,QADI,CACK,IADL,EAEJC,WAFI,GAGJC,QAHI,CAGK,CAHL,EAGQ,GAHR,EAIJP,SAJI,CAIM,CAJN,EAIS,CAJT,CAAP;AAKD,CAND;;AAQA,MAAMQ,YAAY,GAAG,OAAO3D,OAAP,EAAgB4D,QAAhB,EAA0BC,QAA1B,KAAuC;AAC1D;AACA,QAAM3B,UAAU,GAAG,MAAMD,iBAAiB,CAAC4B,QAAQ,CAACC,KAAV,EAAiB,IAAjB,EAAuB,IAAvB,EAA6B,IAA7B,CAA1C;AACA,QAAMP,QAAQ,GAAG,MAAMrB,UAAU,CAAC6B,oBAAX,CAAgCH,QAAhC,CAAvB,CAH0D,CAK1D;;AACA,QAAMI,IAAI,GAAG,CAAC;AACZ9D,IAAAA,MAAM,EAAEqD,QAAQ,CAACrD,MADL;AAEZ+D,IAAAA,MAAM,EAAEX,QAAQ,CAACC,QAAQ,CAACP,GAAV;AAFJ,GAAD,CAAb;AAIA,MAAIkB,aAAa,GAAGX,QAAQ,CAACrD,MAA7B;;AAEA,SAAOgE,aAAa,KAAKhC,UAAzB,EAAqC;AACnC8B,IAAAA,IAAI,CAACG,IAAL,CAAU;AACRjE,MAAAA,MAAM,EAAEgE,aADA;AAERD,MAAAA,MAAM,EAAEX,QAAQ,CAACY,aAAa,CAACE,YAAf;AAFR,KAAV;AAKAF,IAAAA,aAAa,GAAGA,aAAa,CAACG,OAA9B;AACD;;AAEDL,EAAAA,IAAI,CAACxD,OAAL;AACAwD,EAAAA,IAAI,CAAC,CAAD,CAAJ,CAAQvD,IAAR,GAAeoD,QAAf,CAtB0D,CAwB1D;;AACA,OAAK,IAAIS,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGN,IAAI,CAACjB,MAAzB,EAAiCuB,CAAC,EAAlC,EAAsC;AACpC,UAAMC,OAAO,GAAGP,IAAI,CAACM,CAAD,CAApB,CADoC,CAGpC;;AACA,UAAMzB,IAAI,GAAG0B,OAAO,CAAC9D,IAAR,CAAaqD,KAAb,CACVU,MADU,CACH3B,IAAI,IAAIA,IAAI,CAACC,IAAL,CAAUK,SAAV,CAAoB,CAApB,EAAuB,CAAvB,MAA8BoB,OAAO,CAACN,MAD3C,EAEVQ,GAFU,EAAb,CAJoC,CAQpC;;AACA,QAAI,CAAC5B,IAAL,EAAW;AACT;AACArD,MAAAA,GAAG,CAAE,QAAO+E,OAAO,CAACN,MAAO,GAAEL,QAAS,gBAAnC,CAAH,CAFS,CAGT;;AACA;AACD,KAdmC,CAgBpC;;;AACA,QAAIf,IAAI,CAACC,IAAL,KAAe,GAAEyB,OAAO,CAACN,MAAO,GAAEL,QAAS,EAA/C,EAAkD;AAChDpE,MAAAA,GAAG,CAAE,QAAO+E,OAAO,CAACN,MAAO,GAAEL,QAAS,mBAAnC,CAAH,CADgD,CAEhD;AACA;;AACA;AACD,KAtBmC,CAwBpC;;;AACApE,IAAAA,GAAG,CAAE,kBAAiB+E,OAAO,CAACN,MAAO,EAAlC,CAAH;AACA,UAAMxD,IAAI,GAAG,MAAMT,OAAO,CAAC0B,IAAR,CAAagD,GAAb,CAAiB7B,IAAI,CAACQ,IAAtB,CAAnB,CA1BoC,CA4BpC;;AACA,QAAI,CAACW,IAAI,CAACM,CAAC,GAAG,CAAL,CAAT,EAAkB;AAChB9E,MAAAA,GAAG,CAAE,uBAAsB+E,OAAO,CAACN,MAAO,EAAvC,CAAH;AAEA,YAAMhC,iBAAiB,CAACxB,IAAI,CAACqD,KAAN,EAAa5B,UAAb,EAAyBqC,OAAO,CAACrE,MAAjC,EAAyC+C,QAAQ,CAACsB,OAAO,CAACN,MAAT,EAAiB,EAAjB,CAAjD,CAAvB;AACA,YAAMV,QAAQ,GAAG,MAAMrB,UAAU,CAAC6B,oBAAX,CAAgCH,QAAhC,CAAvB,CAJgB,CAMhB;;AACAI,MAAAA,IAAI,CAACG,IAAL,CAAU;AACRjE,QAAAA,MAAM,EAAEqD,QAAQ,CAACrD,MADT;AAER+D,QAAAA,MAAM,EAAEX,QAAQ,CAACC,QAAQ,CAACP,GAAV,CAFR;AAGRvC,QAAAA,IAAI,EAAEA;AAHE,OAAV;AAMA;AACD;;AAED,UAAMkE,WAAW,GAAGX,IAAI,CAACM,CAAC,GAAG,CAAL,CAAxB,CA7CoC,CA+CpC;;AACA,UAAM7B,oBAAoB,CAAChC,IAAI,CAACqD,KAAN,EAAaa,WAAW,CAACzE,MAAzB,EAAiCgC,UAAjC,CAA1B;AAEAyC,IAAAA,WAAW,CAAClE,IAAZ,GAAmBA,IAAnB;AACD;;AAED,QAAMyB,UAAU,CAACP,GAAX,CAAeiC,QAAf,EAAyB,IAAzB,CAAN;AAEAI,EAAAA,IAAI,CAACxD,OAAL;AAEA,SAAO;AACL0B,IAAAA,UADK;AAEL8B,IAAAA;AAFK,GAAP;AAID,CAtFD;;AAwFA,MAAMY,WAAW,GAAG,OAAO5E,OAAP,EAAgB6E,QAAhB,EAA0B1E,OAA1B,KAAsC;AACxD,QAAM2E,KAAK,GAAG,IAAIvF,UAAJ,CAAe;AAC3BwF,IAAAA,IAAI,EAAE,IADqB;AAE3BlE,IAAAA,GAAG,EAAE,IAFsB;AAG3BF,IAAAA,MAAM,EAAE,IAHmB;AAI3BqE,IAAAA,SAAS,EAAE,IAJgB;AAK3BhB,IAAAA,IAAI,EAAE,EALqB;AAM3BiB,IAAAA,KAAK,EAAE,IANoB;AAO3BC,IAAAA,IAAI,EAAE,KAPqB;AAQ3B7D,IAAAA,KAAK,EAAElB,OAAO,CAACkB,KARY;AAS3BD,IAAAA,IAAI,EAAEjB,OAAO,CAACiB;AATa,GAAf,EAUX,EACD,GAAGjB,OADF;AAEDgF,IAAAA,KAAK,EAAE;AAFN,GAVW,CAAd;;AAeA,OAAK,IAAIb,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGO,QAAQ,CAAC9B,MAA7B,EAAqCuB,CAAC,EAAtC,EAA0C;AACxC,UAAMQ,KAAK,CAACM,OAAN,CAAczD,GAAd,CAAkBkD,QAAQ,CAACP,CAAD,CAAR,CAAYe,IAA9B,EAAoC;AACxCrD,MAAAA,IAAI,EAAE6C,QAAQ,CAACP,CAAD,CAAR,CAAYtC,IADsB;AAExCP,MAAAA,GAAG,EAAEoD,QAAQ,CAACP,CAAD,CAAR,CAAY7C;AAFuB,KAApC,CAAN;AAID;;AAED,SAAO5B,IAAI,CAACiF,KAAK,CAAC/C,KAAN,CAAY,EAAZ,EAAgB/B,OAAO,CAACsF,KAAxB,EAA+B,IAA/B,CAAD,CAAX;AACD,CAxBD;;AA0BAC,MAAM,CAACC,OAAP,GAAiB;AACf7B,EAAAA,YADe;AAEf5D,EAAAA,mBAFe;AAGfkC,EAAAA,iBAHe;AAIfQ,EAAAA,oBAJe;AAKfa,EAAAA,QALe;AAMfsB,EAAAA;AANe,CAAjB","sourcesContent":["'use strict'\n\nconst {\n  DAGNode\n} = require('ipld-dag-pb')\nconst Bucket = require('hamt-sharding/src/bucket')\nconst DirSharded = require('ipfs-unixfs-importer/src/dir-sharded')\nconst log = require('debug')('ipfs:mfs:core:utils:hamt-utils')\nconst UnixFS = require('ipfs-unixfs')\nconst mc = require('multicodec')\nconst mh = require('multihashing-async').multihash\nconst last = require('it-last')\nconst { Buffer } = require('buffer')\n\nconst updateHamtDirectory = async (context, links, bucket, options) => {\n  // update parent with new bit field\n  const data = Buffer.from(bucket._children.bitField().reverse())\n  const node = UnixFS.unmarshal(options.parent.Data)\n  const dir = new UnixFS({\n    type: 'hamt-sharded-directory',\n    data,\n    fanout: bucket.tableSize(),\n    hashType: DirSharded.hashFn.code,\n    mode: node.mode,\n    mtime: node.mtime\n  })\n\n  const hashAlg = mh.names[options.hashAlg]\n  const parent = new DAGNode(dir.marshal(), links)\n  const cid = await context.ipld.put(parent, mc.DAG_PB, {\n    cidVersion: options.cidVersion,\n    hashAlg,\n    onlyHash: !options.flush\n  })\n\n  return {\n    node: parent,\n    cid,\n    size: parent.size\n  }\n}\n\nconst recreateHamtLevel = async (links, rootBucket, parentBucket, positionAtParent) => {\n  // recreate this level of the HAMT\n  const bucket = new Bucket({\n    hashFn: DirSharded.hashFn,\n    hash: parentBucket ? parentBucket._options.hash : undefined\n  }, parentBucket, positionAtParent)\n\n  if (parentBucket) {\n    parentBucket._putObjectAt(positionAtParent, bucket)\n  }\n\n  await addLinksToHamtBucket(links, bucket, rootBucket)\n\n  return bucket\n}\n\nconst addLinksToHamtBucket = async (links, bucket, rootBucket) => {\n  await Promise.all(\n    links.map(link => {\n      if (link.Name.length === 2) {\n        const pos = parseInt(link.Name, 16)\n\n        bucket._putObjectAt(pos, new Bucket({\n          hashFn: DirSharded.hashFn\n        }, bucket, pos))\n\n        return Promise.resolve()\n      }\n\n      return (rootBucket || bucket).put(link.Name.substring(2), {\n        size: link.Tsize,\n        cid: link.Hash\n      })\n    })\n  )\n}\n\nconst toPrefix = (position) => {\n  return position\n    .toString('16')\n    .toUpperCase()\n    .padStart(2, '0')\n    .substring(0, 2)\n}\n\nconst generatePath = async (context, fileName, rootNode) => {\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateHamtLevel(rootNode.Links, null, null, null)\n  const position = await rootBucket._findNewBucketAndPos(fileName)\n\n  // the path to the root bucket\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }]\n  let currentBucket = position.bucket\n\n  while (currentBucket !== rootBucket) {\n    path.push({\n      bucket: currentBucket,\n      prefix: toPrefix(currentBucket._posAtParent)\n    })\n\n    currentBucket = currentBucket._parent\n  }\n\n  path.reverse()\n  path[0].node = rootNode\n\n  // load DAGNode for each path segment\n  for (let i = 0; i < path.length; i++) {\n    const segment = path[i]\n\n    // find prefix in links\n    const link = segment.node.Links\n      .filter(link => link.Name.substring(0, 2) === segment.prefix)\n      .pop()\n\n    // entry was not in shard\n    if (!link) {\n      // reached bottom of tree, file will be added to the current bucket\n      log(`Link ${segment.prefix}${fileName} will be added`)\n      // return path\n      continue\n    }\n\n    // found entry\n    if (link.Name === `${segment.prefix}${fileName}`) {\n      log(`Link ${segment.prefix}${fileName} will be replaced`)\n      // file already existed, file will be added to the current bucket\n      // return path\n      continue\n    }\n\n    // found subshard\n    log(`Found subshard ${segment.prefix}`)\n    const node = await context.ipld.get(link.Hash)\n\n    // subshard hasn't been loaded, descend to the next level of the HAMT\n    if (!path[i + 1]) {\n      log(`Loaded new subshard ${segment.prefix}`)\n\n      await recreateHamtLevel(node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16))\n      const position = await rootBucket._findNewBucketAndPos(fileName)\n\n      // i--\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: node\n      })\n\n      continue\n    }\n\n    const nextSegment = path[i + 1]\n\n    // add intermediate links to bucket\n    await addLinksToHamtBucket(node.Links, nextSegment.bucket, rootBucket)\n\n    nextSegment.node = node\n  }\n\n  await rootBucket.put(fileName, true)\n\n  path.reverse()\n\n  return {\n    rootBucket,\n    path\n  }\n}\n\nconst createShard = async (context, contents, options) => {\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: null,\n    parentKey: null,\n    path: '',\n    dirty: true,\n    flat: false,\n    mtime: options.mtime,\n    mode: options.mode\n  }, {\n    ...options,\n    codec: 'dag-pb'\n  })\n\n  for (let i = 0; i < contents.length; i++) {\n    await shard._bucket.put(contents[i].name, {\n      size: contents[i].size,\n      cid: contents[i].cid\n    })\n  }\n\n  return last(shard.flush('', context.block, null))\n}\n\nmodule.exports = {\n  generatePath,\n  updateHamtDirectory,\n  recreateHamtLevel,\n  addLinksToHamtBucket,\n  toPrefix,\n  createShard\n}\n"]},"metadata":{},"sourceType":"script"}